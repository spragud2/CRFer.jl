using Flux
using Distributions: Uniform
using BioSequences
using IterTools


"""
    CRF

A Conditional Random Field (CRF). The struct contains a set of parameters
that can be any number of things. By default, the model contains parameters for k-mers in each
state/domain and transition parameters. 

A CRF with just emission and transition features is essentially a
Hidden Markov Model (HMM), except that the model is discriminative (classifier, p(y|x)) rather than
generative p(x,y).

"""
struct CRF
    Î¸_kmers::Vector{Float64}
    Î¸_transition::Vector{Float64}
end 

"""
    CRF(kmer_feature_size,transition_feature_size)

Function to initialize a new, untrained CRF model
"""
CRF(kmer_feature_size::Integer, transition_feature_size::Integer) =
  CRF(Flux.glorot_uniform(kmer_feature_size), Flux.glorot_uniform(transition_feature_size))


"""

(model::CRF)(x,y,states,kmer_features,transition_features,ğŸ¢)

Forward pass of the model and/or negative log-likelihood of the CRF.

Definition the function in this way allows a CRF object to be called as a function.

```julia-repl
> model = CRF(4,4)
> negativeloglikelihood = model(x,y,states,kmer_features,transition_features,ğŸ¢)
```


This function calculates the likelihood of a sequence given the current
parameter values. 

This includes the calculation of the partition function for the CRF model, 
i.e. the normalization constant to transform the score into a proper probability. 

The partition function is calculated using the forward algorithm (as in HMMs) in log-space.

This function therefore is the CRF likelihood function, and returns the negative
log-likelihood, such that we can perform gradient descent to optimize the feature function
parameters

ğŸ¢ -> L2 regularization scale constant. Sets the degree to which we wish to regularlize the parameters. Probably should not be passed in bayesian model?

"""
function (m::CRF)(x,y,
                    states,
                    kmer_features,
                    transition_features;
                    ğŸ¢ = .1,
                )

    ## sequence score given current parameters

    score = âˆ‘(m.Î¸_kmers .* f(x[1],y[1],kmer_features))
    score += âˆ‘(m.Î¸_transition .* f(y[1],y[1],transition_features))
    for t âˆˆ 2:lastindex(x)
        score += âˆ‘(m.Î¸_kmers .* f(x[t],y[t],kmer_features))
        score += âˆ‘(m.Î¸_transition .* f(y[t],y[t-1],transition_features))
    end


    ## partition function

    ## Gets the summed scores across all possible label
    ## sequences, such that the unnormalized score above
    ## becomes proper probability.

    U = vec([dot(m.Î¸_kmers,f(x[1],i,kmer_features)) for i in states])
    T = vec([dot(m.Î¸_transition,f(i,i,transition_features)) for i in states])
    Î±_prior = U+T 

    for t âˆˆ 2:lastindex(x)
        Î±_x = vec([dot(m.Î¸_kmers,f(x[t],j,kmer_features)) for j in states])
        Î±_y = vec([LSE(vec([dot(m.Î¸_transition,f(i,j,transition_features)) for j in states])) for i in states])
        Î±_prior = LSE(Î±_prior) .+ Î±_x .+ Î±_y
    end
    Z = LSE(Î±_prior)

    # calculation in log space, so (score - Z) is 
    # p(y|x,Î¸), or the probability of sequence labels
    # given the input sequence and model parameters Î¸
    return -(score - Z)  + (ğŸ¢ * (âˆ‘(m.Î¸_kmers.^2) + âˆ‘(m.Î¸_transition.^2)))  # per-tok mean negative log likelihood + L2 reg
end


# function viterbi(model::CRF,x)
#     x
# end 


"""
    train!(model,input;args)

Takes an input CRF model and performs gradient descent. Updates model parameters in place over
N training epochs. The CRF likelihood function is convex, however a learning rate should be used
to avoid bouncing around the global minimum.

Input needs to be passed as a tuple that includes 

input = (x,y,labels,features...)


"""
function train!(model::CRF,
                x,
                y,
                states,
                kmer_features,
                transition_features,
                ;
                N = 100,
                ğŸ¢ = .01,
                return_loss_curve = true)
    
    loss_history = Vector{Float32}(undef,N)
    local loss
    gs = gradient(Flux.params(model)) do
        loss = model(x,y,states,kmer_features,transition_features)
        return loss
    end
    loss_history[i] = loss
    for p âˆˆ Flux.params(model)
        p .-= ğŸ¢ * gs[p]
    end

end



