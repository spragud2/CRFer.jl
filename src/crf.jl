using Flux
using Distributions: Uniform
using BioSequences
using IterTools


"""
    CRF

A Conditional Random Field (CRF). The struct contains a set of parameters
that can be any number of things. By default, the model contains parameters for k-mers in each
state/domain and transition parameters. 

A CRF with just emission and transition features is essentially a
Hidden Markov Model (HMM), except that the model is discriminative (classifier, p(y|x)) rather than
generative p(x,y).

"""
struct CRF
    Œ∏_kmers::Vector{Float64}
    Œ∏_transition::Vector{Float64}
end 

"""
    CRF(kmer_feature_size,transition_feature_size)

Function to initialize a new, untrained CRF model
"""
CRF(kmer_feature_size::Integer, transition_feature_size::Integer) =
  CRF(Flux.glorot_uniform(kmer_feature_size), Flux.glorot_uniform(transition_feature_size))


"""

(model::CRF)(x,y,states,kmer_features,transition_features,üê¢)

Forward pass of the model and/or negative log-likelihood of the CRF.

Definition the function in this way allows a CRF object to be called as a function.

```julia-repl
> model = CRF(4,4)
> negativeloglikelihood = model(x,y,states,kmer_features,transition_features,üê¢)
```


This function calculates the likelihood of a sequence given the current
parameter values. 

This includes the calculation of the partition function for the CRF model, 
i.e. the normalization constant to transform the score into a proper probability. 

The partition function is calculated using the forward algorithm (as in HMMs) in log-space.

This function therefore is the CRF likelihood function, and returns the negative
log-likelihood, such that we can perform gradient descent to optimize the feature function
parameters

üê¢ -> L2 regularization scale constant. Sets the degree to which we wish to regularlize the parameters. Probably should not be passed in bayesian model?

"""
function (m::CRF)(x,y,
                    states,
                    kmer_features,
                    transition_features;
                    üê¢ = .01,
                )

    ## sequence score given current parameters

    score = ‚àë(m.Œ∏_kmers .* f(x[1],y[1],kmer_features))
    score += ‚àë(m.Œ∏_transition .* f(y[1],y[1],transition_features))
    for t ‚àà 2:lastindex(x)
        score += ‚àë(m.Œ∏_kmers .* f(x[t],y[t],kmer_features))
        score += ‚àë(m.Œ∏_transition .* f(y[t],y[t-1],transition_features))
    end


    ## partition function

    ## Gets the summed scores across all possible label
    ## sequences, such that the unnormalized score above
    ## becomes proper probability.

    U = vec([dot(m.Œ∏_kmers,f(x[1],i,kmer_features)) for i in states])
    T = vec([dot(m.Œ∏_transition,f(i,i,transition_features)) for i in states])
    Œ±_prior = U+T 

    for t ‚àà 2:lastindex(x)
        Œ±_x = vec([dot(m.Œ∏_kmers,f(x[t],j,kmer_features)) for j in states])
        Œ±_y = vec([LSE(vec([dot(m.Œ∏_transition,f(i,j,transition_features)) for j in states])) for i in states])
        Œ±_prior = LSE(Œ±_prior) .+ Œ±_x .+ Œ±_y
    end
    Z = LSE(Œ±_prior)

    # calculation in log space, so (score - Z) is 
    # p(y|x,Œ∏), or the probability of sequence labels
    # given the input sequence and model parameters Œ∏
    return -(score - Z)  + (üê¢ * (‚àë(m.Œ∏_kmers.^2) + ‚àë(m.Œ∏_transition.^2)))  # per-tok mean negative log likelihood + L2 reg
end


# function viterbi(model::CRF,x)
#     x
# end 


"""
    train!(model,input;args)

Takes an input CRF model and performs gradient descent. Updates model parameters in place over
N training epochs. The CRF likelihood function is convex, however a learning rate should be used
to avoid bouncing around the global minimum.

Input needs to be passed as a tuple that includes 

input = (x,y,labels,features...)


"""
function train!(model::CRF,
                input;
                N = 100,
                üê¢ = .01,
                return_loss_curve = true)
    
    loss_history = Vector{Float32}(undef,N)

    for i ‚àà 1:N
        local loss
        gs = gradient(Flux.params(model)) do
            loss = model(input...)
            return loss
        end
        loss_history[i] = loss
        for p ‚àà Flux.params(model)
            p .-= üê¢ * gs[p]
        end
    end

end



